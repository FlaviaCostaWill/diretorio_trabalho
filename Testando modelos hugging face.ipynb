{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /home/flavia.costa/.local/lib/python3.8/site-packages (4.35.2)\n",
      "Requirement already satisfied: filelock in /home/flavia.costa/.local/lib/python3.8/site-packages (from transformers) (3.12.2)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /home/flavia.costa/.local/lib/python3.8/site-packages (from transformers) (0.19.4)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/flavia.costa/.local/lib/python3.8/site-packages (from transformers) (1.23.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/flavia.costa/.local/lib/python3.8/site-packages (from transformers) (21.3)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/lib/python3/dist-packages (from transformers) (5.3.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/flavia.costa/.local/lib/python3.8/site-packages (from transformers) (2022.10.31)\n",
      "Requirement already satisfied: requests in /home/flavia.costa/.local/lib/python3.8/site-packages (from transformers) (2.28.2)\n",
      "Requirement already satisfied: tokenizers<0.19,>=0.14 in /home/flavia.costa/.local/lib/python3.8/site-packages (from transformers) (0.15.0)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /home/flavia.costa/.local/lib/python3.8/site-packages (from transformers) (0.4.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/flavia.costa/.local/lib/python3.8/site-packages (from transformers) (4.64.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /home/flavia.costa/.local/lib/python3.8/site-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (2023.12.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/flavia.costa/.local/lib/python3.8/site-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (4.4.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /home/flavia.costa/.local/lib/python3.8/site-packages (from packaging>=20.0->transformers) (3.0.9)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/flavia.costa/.local/lib/python3.8/site-packages (from requests->transformers) (2.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests->transformers) (2.8)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/flavia.costa/.local/lib/python3.8/site-packages (from requests->transformers) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/lib/python3/dist-packages (from requests->transformers) (2019.11.28)\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-28 17:21:54.181097: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-12-28 17:21:54.298957: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-12-28 17:21:54.299557: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-12-28 17:21:55.954078: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing installation: sentence-transformers 2.2.2\n",
      "Uninstalling sentence-transformers-2.2.2:\n",
      "  Would remove:\n",
      "    /home/flavia.costa/.local/lib/python3.8/site-packages/sentence_transformers-2.2.2.dist-info/*\n",
      "    /home/flavia.costa/.local/lib/python3.8/site-packages/sentence_transformers/*\n",
      "Proceed (y/n)? ^C\n",
      "\u001b[31mERROR: Operation cancelled by user\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip uninstall sentence-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d5c7277dc1c4d419b4b8b92555103c8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       ".gitattributes:   0%|          | 0.00/1.48k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "844b07c3b467486d922082263c6abf49",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "1_Pooling/config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4dc630647a424139a668b50fd2470403",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/9.80k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be2b5c1c3a0a4c4b9449ba6517fcfb77",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "added_tokens.json:   0%|          | 0.00/2.88k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b55b66d2edd442aaa39d1158e9731922",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/898 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b8996bdcff4842d9a6c6d2abc1eb29df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config_sentence_transformers.json:   0%|          | 0.00/123 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cb3a334dc2e4486096ab693283eeda82",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/490k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fcf18166d6794a9b9005958c8243d2a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/204M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c0081b45afd14fedb7d90ddcfc337365",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentence_bert_config.json:   0%|          | 0.00/54.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c23a8180a37421db2f6510eb83d34c8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/2.48k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "20f70e5079c845bca540316c45f72663",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/2.19M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "16b82316f13b49c0ab2ec80e8a33a52a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/289 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b4a5f291116443eb361ddf493d23f73",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/831k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a175a80a694848babd2493eedd21239e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modules.json:   0%|          | 0.00/229 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "ImportError",
     "evalue": "\nT5EncoderModel requires the PyTorch library but it was not found in your environment.\nHowever, we were able to find a TensorFlow installation. TensorFlow classes begin\nwith \"TF\", but are otherwise identically named to our PyTorch classes. This\nmeans that the TF equivalent of the class you tried to import would be \"TFT5EncoderModel\".\nIf you want to use TensorFlow, please use TF classes instead!\n\nIf you really do want to use PyTorch please go to\nhttps://pytorch.org/get-started/locally/ and follow the instructions that\nmatch your environment.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Input \u001b[0;32mIn [4]\u001b[0m, in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msentence_transformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SentenceTransformer\n\u001b[1;32m      3\u001b[0m sentences \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEste é um exemplo de sentença\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mA sentença é convertida em um texto de dimensão 513\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m----> 5\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mSentenceTransformer\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtgsc/sentence-transformer-ult5-pt-small\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m embeddings \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mencode(sentences)\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(embeddings)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/sentence_transformers/SentenceTransformer.py:95\u001b[0m, in \u001b[0;36mSentenceTransformer.__init__\u001b[0;34m(self, model_name_or_path, modules, device, cache_folder, use_auth_token)\u001b[0m\n\u001b[1;32m     87\u001b[0m         snapshot_download(model_name_or_path,\n\u001b[1;32m     88\u001b[0m                             cache_dir\u001b[38;5;241m=\u001b[39mcache_folder,\n\u001b[1;32m     89\u001b[0m                             library_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msentence-transformers\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m     90\u001b[0m                             library_version\u001b[38;5;241m=\u001b[39m__version__,\n\u001b[1;32m     91\u001b[0m                             ignore_files\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mflax_model.msgpack\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrust_model.ot\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtf_model.h5\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m     92\u001b[0m                             use_auth_token\u001b[38;5;241m=\u001b[39muse_auth_token)\n\u001b[1;32m     94\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(model_path, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodules.json\u001b[39m\u001b[38;5;124m'\u001b[39m)):    \u001b[38;5;66;03m#Load as SentenceTransformer model\u001b[39;00m\n\u001b[0;32m---> 95\u001b[0m     modules \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_load_sbert_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     96\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:   \u001b[38;5;66;03m#Load with AutoModel\u001b[39;00m\n\u001b[1;32m     97\u001b[0m     modules \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_load_auto_model(model_path)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/sentence_transformers/SentenceTransformer.py:840\u001b[0m, in \u001b[0;36mSentenceTransformer._load_sbert_model\u001b[0;34m(self, model_path)\u001b[0m\n\u001b[1;32m    838\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m module_config \u001b[38;5;129;01min\u001b[39;00m modules_config:\n\u001b[1;32m    839\u001b[0m     module_class \u001b[38;5;241m=\u001b[39m import_from_string(module_config[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtype\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m--> 840\u001b[0m     module \u001b[38;5;241m=\u001b[39m \u001b[43mmodule_class\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodule_config\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mpath\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    841\u001b[0m     modules[module_config[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m'\u001b[39m]] \u001b[38;5;241m=\u001b[39m module\n\u001b[1;32m    843\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m modules\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/sentence_transformers/models/Transformer.py:137\u001b[0m, in \u001b[0;36mTransformer.load\u001b[0;34m(input_path)\u001b[0m\n\u001b[1;32m    135\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(sbert_config_path) \u001b[38;5;28;01mas\u001b[39;00m fIn:\n\u001b[1;32m    136\u001b[0m     config \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mload(fIn)\n\u001b[0;32m--> 137\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mTransformer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_name_or_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/sentence_transformers/models/Transformer.py:29\u001b[0m, in \u001b[0;36mTransformer.__init__\u001b[0;34m(self, model_name_or_path, max_seq_length, model_args, cache_dir, tokenizer_args, do_lower_case, tokenizer_name_or_path)\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdo_lower_case \u001b[38;5;241m=\u001b[39m do_lower_case\n\u001b[1;32m     28\u001b[0m config \u001b[38;5;241m=\u001b[39m AutoConfig\u001b[38;5;241m.\u001b[39mfrom_pretrained(model_name_or_path, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_args, cache_dir\u001b[38;5;241m=\u001b[39mcache_dir)\n\u001b[0;32m---> 29\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_load_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenizer \u001b[38;5;241m=\u001b[39m AutoTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(tokenizer_name_or_path \u001b[38;5;28;01mif\u001b[39;00m tokenizer_name_or_path \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m model_name_or_path, cache_dir\u001b[38;5;241m=\u001b[39mcache_dir, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mtokenizer_args)\n\u001b[1;32m     33\u001b[0m \u001b[38;5;66;03m#No max_seq_length set. Try to infer from model\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/sentence_transformers/models/Transformer.py:47\u001b[0m, in \u001b[0;36mTransformer._load_model\u001b[0;34m(self, model_name_or_path, config, cache_dir)\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[38;5;124;03m\"\"\"Loads the transformer model\"\"\"\u001b[39;00m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(config, T5Config):\n\u001b[0;32m---> 47\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_load_t5_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     49\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mauto_model \u001b[38;5;241m=\u001b[39m AutoModel\u001b[38;5;241m.\u001b[39mfrom_pretrained(model_name_or_path, config\u001b[38;5;241m=\u001b[39mconfig, cache_dir\u001b[38;5;241m=\u001b[39mcache_dir)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/sentence_transformers/models/Transformer.py:55\u001b[0m, in \u001b[0;36mTransformer._load_t5_model\u001b[0;34m(self, model_name_or_path, config, cache_dir)\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m T5EncoderModel\n\u001b[1;32m     54\u001b[0m T5EncoderModel\u001b[38;5;241m.\u001b[39m_keys_to_ignore_on_load_unexpected \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdecoder.*\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m---> 55\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mauto_model \u001b[38;5;241m=\u001b[39m \u001b[43mT5EncoderModel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m(model_name_or_path, config\u001b[38;5;241m=\u001b[39mconfig, cache_dir\u001b[38;5;241m=\u001b[39mcache_dir)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/transformers/utils/import_utils.py:1259\u001b[0m, in \u001b[0;36mDummyObject.__getattribute__\u001b[0;34m(cls, key)\u001b[0m\n\u001b[1;32m   1257\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m key\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m key \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_from_config\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m   1258\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__getattribute__\u001b[39m(key)\n\u001b[0;32m-> 1259\u001b[0m \u001b[43mrequires_backends\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_backends\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/transformers/utils/import_utils.py:1238\u001b[0m, in \u001b[0;36mrequires_backends\u001b[0;34m(obj, backends)\u001b[0m\n\u001b[1;32m   1236\u001b[0m \u001b[38;5;66;03m# Raise an error for users who might not realize that classes without \"TF\" are torch-only\u001b[39;00m\n\u001b[1;32m   1237\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtorch\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m backends \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtf\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m backends \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_available() \u001b[38;5;129;01mand\u001b[39;00m is_tf_available():\n\u001b[0;32m-> 1238\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(PYTORCH_IMPORT_ERROR_WITH_TF\u001b[38;5;241m.\u001b[39mformat(name))\n\u001b[1;32m   1240\u001b[0m \u001b[38;5;66;03m# Raise the inverse error for PyTorch users trying to load TF classes\u001b[39;00m\n\u001b[1;32m   1241\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtf\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m backends \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtorch\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m backends \u001b[38;5;129;01mand\u001b[39;00m is_torch_available() \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_tf_available():\n",
      "\u001b[0;31mImportError\u001b[0m: \nT5EncoderModel requires the PyTorch library but it was not found in your environment.\nHowever, we were able to find a TensorFlow installation. TensorFlow classes begin\nwith \"TF\", but are otherwise identically named to our PyTorch classes. This\nmeans that the TF equivalent of the class you tried to import would be \"TFT5EncoderModel\".\nIf you want to use TensorFlow, please use TF classes instead!\n\nIf you really do want to use PyTorch please go to\nhttps://pytorch.org/get-started/locally/ and follow the instructions that\nmatch your environment.\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "sentences = [\"Este é um exemplo de sentença\", \"A sentença é convertida em um texto de dimensão 513\"]\n",
    "\n",
    "model = SentenceTransformer('tgsc/sentence-transformer-ult5-pt-small')\n",
    "embeddings = model.encode(sentences)\n",
    "print(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Could not load model textattack/bert-base-uncased-QQP with any of the following classes: (<class 'transformers.models.auto.modeling_tf_auto.TFAutoModelForSequenceClassification'>, <class 'transformers.models.bert.modeling_tf_bert.TFBertForSequenceClassification'>). See the original errors:\n\nwhile loading with TFAutoModelForSequenceClassification, an error is thrown:\nTraceback (most recent call last):\n  File \"/home/flavia.costa/.local/lib/python3.8/site-packages/transformers/pipelines/base.py\", line 269, in infer_framework_load_model\n    model = model_class.from_pretrained(model, **kwargs)\n  File \"/home/flavia.costa/.local/lib/python3.8/site-packages/transformers/models/auto/auto_factory.py\", line 566, in from_pretrained\n    return model_class.from_pretrained(\n  File \"/home/flavia.costa/.local/lib/python3.8/site-packages/transformers/modeling_tf_utils.py\", line 2823, in from_pretrained\n    raise EnvironmentError(\nOSError: textattack/bert-base-uncased-QQP does not appear to have a file named tf_model.h5 but there is a file for PyTorch weights. Use `from_pt=True` to load this model from those weights.\n\nwhile loading with TFBertForSequenceClassification, an error is thrown:\nTraceback (most recent call last):\n  File \"/home/flavia.costa/.local/lib/python3.8/site-packages/transformers/pipelines/base.py\", line 269, in infer_framework_load_model\n    model = model_class.from_pretrained(model, **kwargs)\n  File \"/home/flavia.costa/.local/lib/python3.8/site-packages/transformers/modeling_tf_utils.py\", line 2823, in from_pretrained\n    raise EnvironmentError(\nOSError: textattack/bert-base-uncased-QQP does not appear to have a file named tf_model.h5 but there is a file for PyTorch weights. Use `from_pt=True` to load this model from those weights.\n\n\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[0;32mIn [7]\u001b[0m, in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m pipeline\n\u001b[0;32m----> 3\u001b[0m classifier \u001b[38;5;241m=\u001b[39m \u001b[43mpipeline\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtext-classification\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtextattack/bert-base-uncased-QQP\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrom_pt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m classifier(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWhich city is the capital of France?, Where is the capital of France?\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/transformers/pipelines/__init__.py:870\u001b[0m, in \u001b[0;36mpipeline\u001b[0;34m(task, model, config, tokenizer, feature_extractor, image_processor, framework, revision, use_fast, token, device, device_map, torch_dtype, trust_remote_code, model_kwargs, pipeline_class, **kwargs)\u001b[0m\n\u001b[1;32m    868\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(model, \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m framework \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    869\u001b[0m     model_classes \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtf\u001b[39m\u001b[38;5;124m\"\u001b[39m: targeted_task[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtf\u001b[39m\u001b[38;5;124m\"\u001b[39m], \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m: targeted_task[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m]}\n\u001b[0;32m--> 870\u001b[0m     framework, model \u001b[38;5;241m=\u001b[39m \u001b[43minfer_framework_load_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    871\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    872\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel_classes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_classes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    873\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    874\u001b[0m \u001b[43m        \u001b[49m\u001b[43mframework\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mframework\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    875\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    876\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mhub_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    877\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    878\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    880\u001b[0m model_config \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mconfig\n\u001b[1;32m    881\u001b[0m hub_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_commit_hash\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39m_commit_hash\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/transformers/pipelines/base.py:282\u001b[0m, in \u001b[0;36minfer_framework_load_model\u001b[0;34m(model, config, model_classes, task, framework, **model_kwargs)\u001b[0m\n\u001b[1;32m    280\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m class_name, trace \u001b[38;5;129;01min\u001b[39;00m all_traceback\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m    281\u001b[0m             error \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwhile loading with \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mclass_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, an error is thrown:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mtrace\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 282\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    283\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCould not load model \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m with any of the following classes: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mclass_tuple\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. See the original errors:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00merror\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    284\u001b[0m         )\n\u001b[1;32m    286\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m framework \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    287\u001b[0m     framework \u001b[38;5;241m=\u001b[39m infer_framework(model\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m)\n",
      "\u001b[0;31mValueError\u001b[0m: Could not load model textattack/bert-base-uncased-QQP with any of the following classes: (<class 'transformers.models.auto.modeling_tf_auto.TFAutoModelForSequenceClassification'>, <class 'transformers.models.bert.modeling_tf_bert.TFBertForSequenceClassification'>). See the original errors:\n\nwhile loading with TFAutoModelForSequenceClassification, an error is thrown:\nTraceback (most recent call last):\n  File \"/home/flavia.costa/.local/lib/python3.8/site-packages/transformers/pipelines/base.py\", line 269, in infer_framework_load_model\n    model = model_class.from_pretrained(model, **kwargs)\n  File \"/home/flavia.costa/.local/lib/python3.8/site-packages/transformers/models/auto/auto_factory.py\", line 566, in from_pretrained\n    return model_class.from_pretrained(\n  File \"/home/flavia.costa/.local/lib/python3.8/site-packages/transformers/modeling_tf_utils.py\", line 2823, in from_pretrained\n    raise EnvironmentError(\nOSError: textattack/bert-base-uncased-QQP does not appear to have a file named tf_model.h5 but there is a file for PyTorch weights. Use `from_pt=True` to load this model from those weights.\n\nwhile loading with TFBertForSequenceClassification, an error is thrown:\nTraceback (most recent call last):\n  File \"/home/flavia.costa/.local/lib/python3.8/site-packages/transformers/pipelines/base.py\", line 269, in infer_framework_load_model\n    model = model_class.from_pretrained(model, **kwargs)\n  File \"/home/flavia.costa/.local/lib/python3.8/site-packages/transformers/modeling_tf_utils.py\", line 2823, in from_pretrained\n    raise EnvironmentError(\nOSError: textattack/bert-base-uncased-QQP does not appear to have a file named tf_model.h5 but there is a file for PyTorch weights. Use `from_pt=True` to load this model from those weights.\n\n\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "classifier = pipeline(\"text-classification\", model = \"textattack/bert-base-uncased-QQP\")\n",
    "\n",
    "classifier(\"Which city is the capital of France?, Where is the capital of France?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All PyTorch model weights were used when initializing TFT5ForConditionalGeneration.\n",
      "\n",
      "All the weights of TFT5ForConditionalGeneration were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFT5ForConditionalGeneration for predictions without further training.\n",
      "/home/flavia.costa/.local/lib/python3.8/site-packages/transformers/generation/tf_utils.py:838: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length.  recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "2023-12-05 16:02:24.556261: I tensorflow/compiler/xla/service/service.cc:169] XLA service 0x8764f620 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2023-12-05 16:02:24.556302: I tensorflow/compiler/xla/service/service.cc:177]   StreamExecutor device (0): Host, Default Version\n",
      "2023-12-05 16:02:24.625707: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2023-12-05 16:02:24.819026: I ./tensorflow/compiler/jit/device_compiler.h:180] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'summary_text': \"'Essa é a única promoção no caso ? Dentro dessa plataforma will','\"}]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summarizer = pipeline(\"summarization\", model=\"msarmento/ptt5-small-portuguese-vocab-finetuned-xlsum-pt\")\n",
    "\n",
    "#\"knkarthick/MEETING_SUMMARY\"\n",
    "\n",
    "\n",
    "text = '''\n",
    "\n",
    "'Essa é  a única  promoção no caso ? Dentro dessa plataforma will','Obrigado!! Karen','Sim. Me falaram sobre o pagamento da fatura para vir com o saldo em dobro. É  verdade ?','Hummm! Não sabia que tinha esse canal','Me manda o link para anexar outro','Tem outro. Peraí','Sim. Porquê vc's não  tem um número  para falar direto com vc's da central ? Seria bem melhor','Ok.'\n",
    "'''\n",
    "summarizer(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFBartForConditionalGeneration: ['model.decoder.embed_tokens.weight']\n",
      "- This IS expected if you are initializing TFBartForConditionalGeneration from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBartForConditionalGeneration from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights or buffers of the TF 2.0 model TFBartForConditionalGeneration were not initialized from the PyTorch model and are newly initialized: ['model.shared.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'summary_text': ' elevenPiPiPi punishmentsPiPi bunPiPiECDPi bunECDECDECD bunECD bun bunECD punishmentsECDECD punishments bun bun punishmentsECD bun punishments bunECDLt bun bun bunLtECD bun UW bun bun discl bun bun Offense bun bun Hip bun bun045ECD bun045 bun045 Offense bun045045045 bun Offense045045 discl045045 personalities personalities personalitiesudenceudenceudence personalities personalities forthcoming personalities cease Produ Produ cease cease cease Produ cease journals cease cease ratified ratified ratifiedades Paste cowork Arrow cowork cease cowork simultane cowork hidden cowork cowork cowork ratified ratifiedupper ratified implementing Kirin ratified ratified Passage ratified Passage Passage Passage presents epilepsy presents presents presents ratified presents presents Jazz presents presents epilepsy ratified presents ratified Jazz Passage presentsdream'}]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summarizer = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\")\n",
    "\n",
    "#\"knkarthick/MEETING_SUMMARY\"\n",
    "\n",
    "\n",
    "text = '''\n",
    "\n",
    "Essa é  a única  promoção no caso ? Dentro dessa plataforma will. Obrigado!! Karen, Sim. Me falaram sobre o pagamento da fatura para vir com o saldo em dobro. É  verdade ?', Hummm! Não sabia que tinha esse canal, Me manda o link para anexar outro, Tem outro. Peraí,Sim. Porquê vc's não  tem um número  para falar direto com vc's da central ? Seria bem melhor Ok.\n",
    "\n",
    "'''\n",
    "summarizer(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert-base-uncased-finetuned-sst-2-english and revision af0f99b (https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "All PyTorch model weights were used when initializing TFDistilBertForSequenceClassification.\n",
      "\n",
      "All the weights of TFDistilBertForSequenceClassification were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFDistilBertForSequenceClassification for predictions without further training.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'label': 'POSITIVE', 'score': 0.9998799562454224}]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summarizer = pipeline(\"sentiment-analysis\")\n",
    "\n",
    "text = '''\n",
    "\n",
    "I love it\n",
    "\n",
    "'''\n",
    "\n",
    "summarizer(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testes para modelo de Zero-Shot Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFBartForSequenceClassification: ['model.encoder.version', 'model.decoder.version']\n",
      "- This IS expected if you are initializing TFBartForSequenceClassification from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBartForSequenceClassification from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of TFBartForSequenceClassification were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBartForSequenceClassification for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "pipe = pipeline(model=\"facebook/bart-large-mnli\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "#label = [\"aguardar\", \"negativa\", \"questionamento\", \"seguir\"]\n",
    "#label = [\"permanecer\", \"aguardar\", \"negativa\", \"questionamento\", \"solicitação\"] 0.46\n",
    "#label = [\"aguardar\", \"negativa\", \"questionamento\", \"confirmação\", \"encaminhar\", \"solicitação\"]\n",
    "#se eu mantenho atendimento eu cancelo alguns casos de aguardar\n",
    "\n",
    "label = [\"ganhando tempo\", \"sendo negativo\", \"sendo atencioso\", \"dando início\", \"fazendo perguntas\"]\n",
    "hipotese = \"O agente está {} no atendimento.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "verbalizacoes = [\"Aguarde só um minuto\"\n",
    "                 , \"Mais alguma dúvida, Flávio?\"\n",
    "                 , \"Já disse que isso não é possível\"\n",
    "                 , \"Thaynar, para podermos seguir com o procedimento, peço, por favor, que me envie uma foto selfie segurando uma folha. Nessa folha, você precisa escrever o código M6145534.\"\n",
    "                 , \"Já foi explicado que não.\"\n",
    "                 , \"Não existe mandar senha, Maria. Senha é algo que somente você possui. Não temos acesso.\"\n",
    "                 , \"Ei, tudo bem? Me chamo Anna e vou prosseguir com o atendimento 😊\"\n",
    "                 , \"Eu vou te passar para um colega especialista que ira te auxiliar melhor nesse caso, ok?! Só peço que aguarde um momento, por favor! 😊\"\n",
    "                 , \"Entretanto, para conseguirmos prosseguir com a abertura de análise, será necessário realizar o bloqueio definitivo do cartão e solicitar a segunda via, ao final do atendimento, tudo bem?\"\n",
    "                 , \"Olá, Vinícius, tudo bem? Meu nome é Maysa e te atenderei a partir de agora.🏃‍♀️\"\n",
    "                 , \"Agora, preciso, por gentileza, que você me confirme algumas informações, tá bem?​\"\n",
    "                 , \"Peço que permaneça na tela do chat durante o atendimento para ser mais dinâmico, por favor!!\"\n",
    "                 , \"Aguarde um momento, por favor! É importante que mantenha- se na tela do chat, enquanto realizo algumas verificações.\"\n",
    "                 , \"Me chamo Natalice e irei te ajudar hoje. Peço por gentileza, que permaneça aqui na conversa\"\n",
    "                 , \"Olá, Maria! Tudo bem? Sou Thiago, estou verificando sua dúvida, peço a gentileza de permanecer na tela do nosso chat. Muito obrigado por aguardar, prometo que não demorarei!\"\n",
    "                 , \"Você pode me contar melhor qual a sua dúvida, por gentileza?\"\n",
    "                 , \"Detalhe a sua dúvida. \"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(verbalizacoes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "gabarito = [\"evitativo\"\n",
    "                 , \"atencioso\"\n",
    "                 , \"neutro\"\n",
    "                 , \"negativa\"\n",
    "                 , \"negativa\"\n",
    "                 , \"seguir\"\n",
    "                 , \"seguir\"\n",
    "                 , \"seguir\"\n",
    "                 , \"seguir\"\n",
    "                 , \"seguir\"\n",
    "                 , \"aguardar\"\n",
    "                 , \"aguardar\"\n",
    "                 , \"aguardar\"\n",
    "                 , \"aguardar\"\n",
    "                 , \"questionamento\"\n",
    "                 , \"questionamento\"\n",
    "                 ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "verbalizacoes2 = []\n",
    "\n",
    "for i in verbalizacoes:\n",
    "    x = i.lower()\n",
    "    x = x.replace('tudo bem', '')\n",
    "    x = x.replace('tá bem', '')\n",
    "    x = x.replace('ok', '')\n",
    "    x = x.replace('?', '')\n",
    "    verbalizacoes2.append(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['aguarde só um minuto', 'mais alguma dúvida, flávio', 'thaynar, para podermos seguir com o procedimento, peço, por favor, que me envie uma foto selfie segurando uma folha. nessa folha, você precisa escrever o código m6145534.', 'já foi explicado que não.', 'não existe mandar senha, maria. senha é algo que somente você possui. não temos acesso.', 'ei,  me chamo anna e vou prosseguir com o atendimento 😊', 'eu vou te passar para um colega especialista que ira te auxiliar melhor nesse caso, ! só peço que aguarde um momento, por favor! 😊', 'entretanto, para conseguirmos prosseguir com a abertura de análise, será necessário realizar o bloqueio definitivo do cartão e solicitar a segunda via, ao final do atendimento, ', 'olá, vinícius,  meu nome é maysa e te atenderei a partir de agora.🏃\\u200d♀️', 'agora, preciso, por gentileza, que você me confirme algumas informações, \\u200b', 'peço que permaneça na tela do chat durante o atendimento para ser mais dinâmico, por favor!!', 'aguarde um momento, por favor! é importante que mantenha- se na tela do chat, enquanto realizo algumas verificações.', 'me chamo natalice e irei te ajudar hoje. peço por gentileza, que permaneça aqui na conversa', 'olá, maria!  sou thiago, estou verificando sua dúvida, peço a gentileza de permanecer na tela do nosso chat. muito obrigado por aguardar, prometo que não demorarei!', 'você pode me contar melhor qual a sua dúvida, por gentileza', 'detalhe a sua dúvida. ']\n"
     ]
    }
   ],
   "source": [
    "print(verbalizacoes2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sequence': 'não existe mandar senha, maria. senha é algo que somente você possui. não temos acesso.',\n",
       " 'labels': ['sendo negativo',\n",
       "  'fazendo perguntas',\n",
       "  'dando início',\n",
       "  'sendo atencioso',\n",
       "  'ganhando tempo'],\n",
       " 'scores': [0.23959803581237793,\n",
       "  0.23386351764202118,\n",
       "  0.1837989091873169,\n",
       "  0.18038825690746307,\n",
       "  0.16235123574733734]}"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe(verbalizacoes2[5],\n",
    "    candidate_labels=label,\n",
    "    hypothesis_template=hipotese)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_23381/3671424338.py:7: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df_scores = df_scores.append(temp)\n",
      "/tmp/ipykernel_23381/3671424338.py:7: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df_scores = df_scores.append(temp)\n",
      "/tmp/ipykernel_23381/3671424338.py:7: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df_scores = df_scores.append(temp)\n",
      "/tmp/ipykernel_23381/3671424338.py:7: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df_scores = df_scores.append(temp)\n",
      "/tmp/ipykernel_23381/3671424338.py:7: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df_scores = df_scores.append(temp)\n",
      "/tmp/ipykernel_23381/3671424338.py:7: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df_scores = df_scores.append(temp)\n",
      "/tmp/ipykernel_23381/3671424338.py:7: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df_scores = df_scores.append(temp)\n",
      "/tmp/ipykernel_23381/3671424338.py:7: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df_scores = df_scores.append(temp)\n",
      "/tmp/ipykernel_23381/3671424338.py:7: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df_scores = df_scores.append(temp)\n",
      "/tmp/ipykernel_23381/3671424338.py:7: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df_scores = df_scores.append(temp)\n",
      "/tmp/ipykernel_23381/3671424338.py:7: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df_scores = df_scores.append(temp)\n",
      "/tmp/ipykernel_23381/3671424338.py:7: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df_scores = df_scores.append(temp)\n",
      "/tmp/ipykernel_23381/3671424338.py:7: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df_scores = df_scores.append(temp)\n",
      "/tmp/ipykernel_23381/3671424338.py:7: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df_scores = df_scores.append(temp)\n"
     ]
    }
   ],
   "source": [
    "df_scores = pd.DataFrame()\n",
    "\n",
    "for i in verbalizacoes2:\n",
    "    prediction = pipe(i,\n",
    "    candidate_labels=label,)\n",
    "    temp = pd.DataFrame(prediction)\n",
    "    df_scores = df_scores.append(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sequence</th>\n",
       "      <th>labels</th>\n",
       "      <th>scores</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>aguarde só um minuto</td>\n",
       "      <td>aguardar</td>\n",
       "      <td>0.634727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>aguarde só um minuto</td>\n",
       "      <td>seguir</td>\n",
       "      <td>0.253456</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>aguarde só um minuto</td>\n",
       "      <td>questionamento</td>\n",
       "      <td>0.093330</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>aguarde só um minuto</td>\n",
       "      <td>negativa</td>\n",
       "      <td>0.018487</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>mais alguma dúvida, flávio</td>\n",
       "      <td>negativa</td>\n",
       "      <td>0.408188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>você pode me contar melhor qual a sua dúvida, ...</td>\n",
       "      <td>negativa</td>\n",
       "      <td>0.020713</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>detalhe a sua dúvida.</td>\n",
       "      <td>questionamento</td>\n",
       "      <td>0.526641</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>detalhe a sua dúvida.</td>\n",
       "      <td>seguir</td>\n",
       "      <td>0.316473</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>detalhe a sua dúvida.</td>\n",
       "      <td>aguardar</td>\n",
       "      <td>0.086768</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>detalhe a sua dúvida.</td>\n",
       "      <td>negativa</td>\n",
       "      <td>0.070118</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>64 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             sequence          labels  \\\n",
       "0                                aguarde só um minuto        aguardar   \n",
       "1                                aguarde só um minuto          seguir   \n",
       "2                                aguarde só um minuto  questionamento   \n",
       "3                                aguarde só um minuto        negativa   \n",
       "0                          mais alguma dúvida, flávio        negativa   \n",
       "..                                                ...             ...   \n",
       "3   você pode me contar melhor qual a sua dúvida, ...        negativa   \n",
       "0                              detalhe a sua dúvida.   questionamento   \n",
       "1                              detalhe a sua dúvida.           seguir   \n",
       "2                              detalhe a sua dúvida.         aguardar   \n",
       "3                              detalhe a sua dúvida.         negativa   \n",
       "\n",
       "      scores  \n",
       "0   0.634727  \n",
       "1   0.253456  \n",
       "2   0.093330  \n",
       "3   0.018487  \n",
       "0   0.408188  \n",
       "..       ...  \n",
       "3   0.020713  \n",
       "0   0.526641  \n",
       "1   0.316473  \n",
       "2   0.086768  \n",
       "3   0.070118  \n",
       "\n",
       "[64 rows x 3 columns]"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_scores_top = df_scores.query('scores >= 0.40')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "gab_df = pd.DataFrame()\n",
    "gab_df['sequence'] = verbalizacoes2\n",
    "gab_df['label_correta'] = gabarito"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_scores_top = df_scores_top.reset_index().join(gab_df.set_index('sequence'), on='sequence', how = 'left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_scores_top['correto'] = np.where(df_scores_top['labels'] == df_scores_top['label_correta'], 1, 0 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6875\n"
     ]
    }
   ],
   "source": [
    "print(df_scores_top['correto'].sum() / len(verbalizacoes) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>sequence</th>\n",
       "      <th>labels</th>\n",
       "      <th>scores</th>\n",
       "      <th>label_correta</th>\n",
       "      <th>correto</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>aguarde só um minuto</td>\n",
       "      <td>aguardar</td>\n",
       "      <td>0.634727</td>\n",
       "      <td>aguardar</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>mais alguma dúvida, flávio</td>\n",
       "      <td>negativa</td>\n",
       "      <td>0.408188</td>\n",
       "      <td>questionamento</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>thaynar, para podermos seguir com o procedimen...</td>\n",
       "      <td>seguir</td>\n",
       "      <td>0.862042</td>\n",
       "      <td>seguir</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>não existe mandar senha, maria. senha é algo q...</td>\n",
       "      <td>negativa</td>\n",
       "      <td>0.512174</td>\n",
       "      <td>negativa</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>ei,  me chamo anna e vou prosseguir com o aten...</td>\n",
       "      <td>seguir</td>\n",
       "      <td>0.730494</td>\n",
       "      <td>seguir</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>eu vou te passar para um colega especialista q...</td>\n",
       "      <td>aguardar</td>\n",
       "      <td>0.633276</td>\n",
       "      <td>seguir</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>entretanto, para conseguirmos prosseguir com a...</td>\n",
       "      <td>seguir</td>\n",
       "      <td>0.590568</td>\n",
       "      <td>seguir</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>olá, vinícius,  meu nome é maysa e te atendere...</td>\n",
       "      <td>seguir</td>\n",
       "      <td>0.445991</td>\n",
       "      <td>seguir</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0</td>\n",
       "      <td>agora, preciso, por gentileza, que você me con...</td>\n",
       "      <td>questionamento</td>\n",
       "      <td>0.435435</td>\n",
       "      <td>seguir</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0</td>\n",
       "      <td>peço que permaneça na tela do chat durante o a...</td>\n",
       "      <td>seguir</td>\n",
       "      <td>0.416826</td>\n",
       "      <td>aguardar</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0</td>\n",
       "      <td>aguarde um momento, por favor! é importante qu...</td>\n",
       "      <td>aguardar</td>\n",
       "      <td>0.573356</td>\n",
       "      <td>aguardar</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0</td>\n",
       "      <td>me chamo natalice e irei te ajudar hoje. peço ...</td>\n",
       "      <td>aguardar</td>\n",
       "      <td>0.782117</td>\n",
       "      <td>aguardar</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0</td>\n",
       "      <td>olá, maria!  sou thiago, estou verificando sua...</td>\n",
       "      <td>aguardar</td>\n",
       "      <td>0.512816</td>\n",
       "      <td>aguardar</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0</td>\n",
       "      <td>você pode me contar melhor qual a sua dúvida, ...</td>\n",
       "      <td>questionamento</td>\n",
       "      <td>0.558451</td>\n",
       "      <td>questionamento</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0</td>\n",
       "      <td>detalhe a sua dúvida.</td>\n",
       "      <td>questionamento</td>\n",
       "      <td>0.526641</td>\n",
       "      <td>questionamento</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    index                                           sequence          labels  \\\n",
       "0       0                               aguarde só um minuto        aguardar   \n",
       "1       0                         mais alguma dúvida, flávio        negativa   \n",
       "2       0  thaynar, para podermos seguir com o procedimen...          seguir   \n",
       "3       0  não existe mandar senha, maria. senha é algo q...        negativa   \n",
       "4       0  ei,  me chamo anna e vou prosseguir com o aten...          seguir   \n",
       "5       0  eu vou te passar para um colega especialista q...        aguardar   \n",
       "6       0  entretanto, para conseguirmos prosseguir com a...          seguir   \n",
       "7       0  olá, vinícius,  meu nome é maysa e te atendere...          seguir   \n",
       "8       0  agora, preciso, por gentileza, que você me con...  questionamento   \n",
       "9       0  peço que permaneça na tela do chat durante o a...          seguir   \n",
       "10      0  aguarde um momento, por favor! é importante qu...        aguardar   \n",
       "11      0  me chamo natalice e irei te ajudar hoje. peço ...        aguardar   \n",
       "12      0  olá, maria!  sou thiago, estou verificando sua...        aguardar   \n",
       "13      0  você pode me contar melhor qual a sua dúvida, ...  questionamento   \n",
       "14      0                             detalhe a sua dúvida.   questionamento   \n",
       "\n",
       "      scores   label_correta  correto  \n",
       "0   0.634727        aguardar        1  \n",
       "1   0.408188  questionamento        0  \n",
       "2   0.862042          seguir        1  \n",
       "3   0.512174        negativa        1  \n",
       "4   0.730494          seguir        1  \n",
       "5   0.633276          seguir        0  \n",
       "6   0.590568          seguir        1  \n",
       "7   0.445991          seguir        1  \n",
       "8   0.435435          seguir        0  \n",
       "9   0.416826        aguardar        0  \n",
       "10  0.573356        aguardar        1  \n",
       "11  0.782117        aguardar        1  \n",
       "12  0.512816        aguardar        1  \n",
       "13  0.558451  questionamento        1  \n",
       "14  0.526641  questionamento        1  "
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_scores_top"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_scores.to_csv('frases_agentes.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
